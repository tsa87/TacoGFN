{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsa87/anaconda3/envs/tacogfn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.tacogfn.tasks import pharmaco_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 15016 ids for train\n",
      "loaded 100 ids for test\n",
      "loaded 15207 ids for train\n",
      "loaded 100 ids for test\n",
      "\n",
      "\n",
      "Hyperparameters:\n",
      "\n",
      "log_dir: ./logs/20240121-crossdocked-mo-256-pocket_graph\n",
      "logger: wandb\n",
      "device: cuda\n",
      "split_file: dataset/split_by_name.pt\n",
      "pharmaco_db: misc/pharmacophores_db.lmdb\n",
      "pocket_db: misc/pocket_db.lmdb\n",
      "dock_pharmaco: dataset/affinity_prediction_pharmacophores/crossdocked_dim_256\n",
      "dock_proxy: model_weights/crossdocked_dim_256.pth\n",
      "avg_score: dataset/pocket_to_avg_zinc_vina_score.pt\n",
      "info_only_dock_proxy: model_weights/zinc_1000_dim_128.pth\n",
      "info_only_dock_pharmaco: dataset/affinity_prediction_pharmacophores/zinc_1000_dim_128\n",
      "seed: 0\n",
      "validate_every: 1000\n",
      "checkpoint_every: null\n",
      "print_every: 100\n",
      "start_at_step: 0\n",
      "num_final_gen_steps: null\n",
      "num_training_steps: 50000\n",
      "num_workers: 8\n",
      "hostname: d26630badf49\n",
      "pickle_mp_messages: false\n",
      "git_hash: '0345009'\n",
      "overwrite_existing_exp: true\n",
      "algo:\n",
      "  method: TB\n",
      "  global_batch_size: 16\n",
      "  max_len: 128\n",
      "  max_nodes: 9\n",
      "  max_edges: 128\n",
      "  illegal_action_logreward: -75.0\n",
      "  offline_ratio: 0.0\n",
      "  valid_offline_ratio: 0.0\n",
      "  train_random_action_prob: 0.01\n",
      "  valid_random_action_prob: 0.0\n",
      "  valid_sample_cond_info: true\n",
      "  sampling_tau: 0.99\n",
      "  tb:\n",
      "    bootstrap_own_reward: false\n",
      "    epsilon: null\n",
      "    reward_loss_multiplier: 1.0\n",
      "    variant: TB\n",
      "    do_correct_idempotent: false\n",
      "    do_parameterize_p_b: false\n",
      "    do_sample_p_b: true\n",
      "    do_length_normalize: false\n",
      "    subtb_max_len: 128\n",
      "    Z_learning_rate: 0.001\n",
      "    Z_lr_decay: 50000.0\n",
      "    cum_subtb: true\n",
      "model:\n",
      "  num_layers: 2\n",
      "  num_emb: 256\n",
      "  dropout: 0.0\n",
      "  graph_transformer:\n",
      "    num_heads: 2\n",
      "    ln_type: pre\n",
      "    num_mlp_layers: 0\n",
      "  pharmaco_cond:\n",
      "    pharmaco_dim: 128\n",
      "opt:\n",
      "  opt: adam\n",
      "  learning_rate: 0.0001\n",
      "  lr_decay: 20000.0\n",
      "  weight_decay: 1.0e-08\n",
      "  momentum: 0.9\n",
      "  clip_grad_type: norm\n",
      "  clip_grad_param: 10.0\n",
      "  adam_eps: 1.0e-08\n",
      "replay:\n",
      "  use: false\n",
      "  capacity: 10000\n",
      "  warmup: 1000\n",
      "  hindsight_ratio: 0.0\n",
      "task:\n",
      "  seh: {}\n",
      "  seh_moo:\n",
      "    use_steer_thermometer: false\n",
      "    preference_type: dirichlet\n",
      "    focus_type: null\n",
      "    focus_dirs_listed: null\n",
      "    focus_cosim: 0.0\n",
      "    focus_limit_coef: 1.0\n",
      "    focus_model_training_limits: null\n",
      "    focus_model_state_space_res: null\n",
      "    max_train_it: null\n",
      "    n_valid: 15\n",
      "    n_valid_repeats: 128\n",
      "    objectives:\n",
      "    - seh\n",
      "    - qed\n",
      "    - sa\n",
      "    - mw\n",
      "  pharmaco_frag:\n",
      "    fragment_type: zinc250k_50cutoff_brics\n",
      "    affinity_predictor: alpha\n",
      "    max_qed_reward: 0.7\n",
      "    qed_exponent: 1.0\n",
      "    max_sa_reward: 0.8\n",
      "    sa_exponent: 1.0\n",
      "    max_dock_reward: -5.0\n",
      "    mol_adj: 0.0\n",
      "    leaky_coefficient: 0.2\n",
      "    reward_multiplier: 3.0\n",
      "    ablation: pocket_graph\n",
      "    objectives:\n",
      "    - docking\n",
      "    - qed\n",
      "    - sa\n",
      "cond:\n",
      "  temperature:\n",
      "    sample_dist: uniform\n",
      "    dist_params:\n",
      "    - 0\n",
      "    - 64.0\n",
      "    num_thermometer_dim: 32\n",
      "  moo:\n",
      "    num_objectives: 2\n",
      "    num_thermometer_dim: 16\n",
      "  weighted_prefs:\n",
      "    preference_type: dirichlet\n",
      "  focus_region:\n",
      "    focus_type: learned-tabular\n",
      "    use_steer_thermomether: false\n",
      "    focus_cosim: 0.98\n",
      "    focus_limit_coef: 0.1\n",
      "    focus_model_training_limits:\n",
      "    - 0.25\n",
      "    - 0.75\n",
      "    focus_model_state_space_res: 30\n",
      "    max_train_it: 20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PocketConditionalGraphTransformerGFN(\n",
       "  (transf): PocketConditionalGraphTransformer(\n",
       "    (pocket_encoder): GVP_embedding(\n",
       "      (W_s): Embedding(20, 20)\n",
       "      (W_v): Sequential(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((26,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=3, out_features=16, bias=False)\n",
       "          (ws): Linear(in_features=42, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=16, out_features=16, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (W_e): Sequential(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=1, out_features=1, bias=False)\n",
       "          (ws): Linear(in_features=33, out_features=32, bias=True)\n",
       "          (wv): Linear(in_features=1, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): GVPConvLayer(\n",
       "          (conv): GVPConv()\n",
       "          (norm): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "            (1): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "          )\n",
       "          (ff_func): Sequential(\n",
       "            (0): GVP(\n",
       "              (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "            )\n",
       "            (1): GVP(\n",
       "              (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): GVPConvLayer(\n",
       "          (conv): GVPConv()\n",
       "          (norm): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "            (1): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "          )\n",
       "          (ff_func): Sequential(\n",
       "            (0): GVP(\n",
       "              (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "            )\n",
       "            (1): GVP(\n",
       "              (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): GVPConvLayer(\n",
       "          (conv): GVPConv()\n",
       "          (norm): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): LayerNorm(\n",
       "              (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (dropout): ModuleList(\n",
       "            (0): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "            (1): Dropout(\n",
       "              (sdropout): Dropout(p=0.1, inplace=False)\n",
       "              (vdropout): _VDropout()\n",
       "            )\n",
       "          )\n",
       "          (ff_func): Sequential(\n",
       "            (0): GVP(\n",
       "              (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "            )\n",
       "            (1): GVP(\n",
       "              (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "              (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "              (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (W_out): Sequential(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (ws): Linear(in_features=144, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (graph_transformer): GraphTransformer(\n",
       "      (x2h): Sequential(\n",
       "        (0): Linear(in_features=73, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (e2h): Sequential(\n",
       "        (0): Linear(in_features=14, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (c2h): Sequential(\n",
       "        (0): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (graph2emb): ModuleList(\n",
       "        (0): GENConv(256, 256, aggr=add)\n",
       "        (1): TransformerConv(512, 256, heads=2)\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (3): LayerNorm(256, affine=False, mode=graph)\n",
       "        (4): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (5): LayerNorm(256, affine=False, mode=graph)\n",
       "        (6): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (7): GENConv(256, 256, aggr=add)\n",
       "        (8): TransformerConv(512, 256, heads=2)\n",
       "        (9): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (10): LayerNorm(256, affine=False, mode=graph)\n",
       "        (11): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (12): LayerNorm(256, affine=False, mode=graph)\n",
       "        (13): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlps): ModuleDict(\n",
       "    (stop): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (add_node): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=72, bias=True)\n",
       "    )\n",
       "    (set_edge_attr): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (emb2graph_out): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (logZ): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state = torch.load('logs/20240121-crossdocked-mo-256-pocket_graph/model_state_29000.pt')\n",
    "config = dict(model_state['cfg'])\n",
    "\n",
    "trail = pharmaco_frag.PharmacophoreTrainer(config)\n",
    "trail.model.load_state_dict(model_state['models_state_dict'][0])\n",
    "trail.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "temp = 1.0\n",
    "\n",
    "pharmacophore_idxs = [99] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = (\n",
    "    torch.rand(n) * trail.cfg.cond.temperature.dist_params[1] \n",
    "    # + torch.ones(n) * self.cfg.cond.temperature.dist_params[1] * 3 / 4\n",
    ")  # default to sampling from the upper 3/4 of the temperature range\n",
    "\n",
    "cond_info = {\n",
    "    \"encoding\": trail.task.temperature_conditional.encode(temperatures),\n",
    "    \"pharmacophore\": torch.as_tensor(pharmacophore_idxs),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    trail.algo.graph_sampler.sample_temp = temp\n",
    "    trajs = trail.algo.graph_sampler.sample_from_model(\n",
    "        trail.model,\n",
    "        n, \n",
    "        cond_info,\n",
    "        'cpu')\n",
    "    trail.algo.graph_sampler.sample_temp = 1.0\n",
    "    \n",
    "mols = [trail.ctx.graph_to_mol(traj[\"result\"]) for traj in trajs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trajs, 'random_trajs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tacogfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
